import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
import sklearn

from sklearn.metrics import accuracy_score

"""
MLF_GP1_CreditScore contains 1700 observations of 26 financial and accounting metrics changes 
for a set of firms in several different industries.
The Class label is the Moody's credit rating assigned to the firm in the following quarter.  
Certain ratings are considered "Investment Grade" (=1), other ratings are not (=0) and consequently 
may not be held in certain institutional portfolios (pension plans, etc.)
This is a classification problem; using the features, X, and the machine learning techniques 
from this class, classify the Moody's score (multiclass classification) and the Investment Grade 
(binary classification). Not both at the same time!  Two models - one multiclass, one binary class.



"""

Credit_Score=pd.read_csv("MLF_GP1_CreditScore.csv")

print(Credit_Score.head())

print(Credit_Score.describe())

"""
see here for exploratory
https://github.com/yohanesusanto/IE517_finalproject/blob/master/Project1.ipynb
"""


features_Credit_Score = Credit_Score.iloc[:,0:26]

target_Investment_Grade = Credit_Score.iloc[:,26]


# binary classification for Investment Grade

X_train, X_test, y_train, y_test = train_test_split(features_Credit_Score, target_Investment_Grade,test_size=0.2, random_state=1,stratify=target_Investment_Grade)

scaler = preprocessing.StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)


#PCA 
from sklearn.decomposition import PCA

# explained variance plot befor PCA
cov_matrix=np.cov(X_train.T)
eigenvalues,eigenvectors=np.linalg.eig(cov_matrix)

#print(eigenvalues)
sorted(eigenvalues,reverse=True)

tot = sum(eigenvalues)

part=0
dimension=0

# 95% is set for principal components choice
while part/tot<0.95:
    part=part+eigenvalues[dimension]
    dimension=dimension+1

percentage=part/tot


print("we need",dimension,"components to get 95% or above of the information")

var_exp = [(i / tot) for i in sorted(eigenvalues, reverse=True)]
cum_var_exp = np.cumsum(var_exp)

plt.figure(figsize=(10,5))
plt.bar(range(1, 27), var_exp, alpha=0.5, align='center',label='individual explained variance')
plt.step(range(1, 27), cum_var_exp, where='mid',label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
plt.legend(loc='best')
plt.tight_layout()
plt.show()

#PCA transformation
pca=PCA(n_components=15)

X_train=pca.fit_transform(X_train)
X_test=pca.transform(X_test)

# we only take 15 components for training
#X_train, X_test is already transformed


from sklearn.model_selection import GridSearchCV


# model A: KNN
print("KNN classification")
from sklearn.neighbors import KNeighborsClassifier

knn=KNeighborsClassifier()

parameters={"n_neighbors":range(1,25)}

knn_grid=GridSearchCV(knn,parameters,cv=10)

knn_grid.fit(X_train,y_train)

print("the best n_neighbors is",knn_grid.best_params_["n_neighbors"])

print("in_sample accuracy is",knn_grid.best_score_)

y_predict=knn_grid.predict(X_test)

print("out_of_sample accuracy is",accuracy_score(y_test,y_predict))

print("\n")

# model B: Decision Tree
print("Decision Tree classification")
from sklearn.tree import DecisionTreeClassifier

parameters={"criterion":["gini","entropy"],"max_depth":range(1,21)}

tree=DecisionTreeClassifier()

tree_grid=GridSearchCV(tree,parameters,cv=10)

tree_grid.fit(X_train,y_train)

print("the best parameters are",tree_grid.best_params_)

print("in_sample accuracy is",tree_grid.best_score_)

y_predict = tree_grid.predict(X_test)

print("the out_of_sample accurancy is",accuracy_score(y_test,y_predict))


print("\n")


#modle C: Logistic Regression

print("Logistic Regression")
from sklearn.linear_model import LogisticRegression

#in all solvers, only liblinear support L1 penalty

log_reg=LogisticRegression(solver="liblinear")

parameters={"penalty":["l1","l2"]}

log_reg_grid=GridSearchCV(log_reg,parameters,cv=10)

log_reg_grid.fit(X_train,y_train)

print("the best parameters are",log_reg_grid.best_params_)

print("in_sample accuracy is",log_reg_grid.best_score_)

y_predict = log_reg_grid.predict(X_test)

print("the out_of_sample accurancy is",accuracy_score(y_test,y_predict))

print("\n")


# ensembling

print("Random Forest")

from sklearn.ensemble import RandomForestClassifier

forest=RandomForestClassifier(n_jobs=-1)
parameters={"criterion":["gini","entropy"],"n_estimators":[10,25,50,100,250,500,750,1000],"max_depth":range(1,21)}

forest_grid=GridSearchCV(forest,parameters,cv=10)

forest_grid.fit(X_train,y_train)

print("the best parameters are",forest_grid.best_params_)

print("in_sample accuracy is",forest_grid.best_score_)

y_predict = forest_grid.predict(X_test)

print("the out_of_sample accurancy is",accuracy_score(y_test,y_predict))

